{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-05 14:27:19,658 loading file /Users/cholian/.flair/models/pos-english/a9a73f6cd878edce8a0fa518db76f441f1cc49c2525b2b4557af278ec2f0659e.121306ea62993d04cd1978398b68396931a39eb47754c8a06a87f325ea70ac63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "# ==============================flair models=================================\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/pos-english\")\n",
    "\n",
    "# ==========================transformers models=============================\n",
    "from transformers import pipeline,AutoTokenizer, AutoModelForSeq2SeqLM,AutoModelForQuestionAnswering\n",
    "\n",
    "# classifier_zero_shot_cf = pipeline(\"zero-shot-classification\",\n",
    "#                       model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Q-A(blenderbot)\n",
    "tokenizer_QA = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "model_QA = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "# Summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
    "\n",
    "# Q-A(roberta)\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "\n",
    "# sentiment -distilbert-base-uncased-finetuned-sst-2-english\n",
    "classifier_sentiment = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "\n",
    "# ======================sentence_transformers models=========================\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model_sentence_sim = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import bs4\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the keywords in google\n",
    "def google_search(tokens):\n",
    "    \n",
    "    text= f\"What is {tokens}?\"\n",
    "    url = 'https://google.com/search?q=' + text\n",
    "    \n",
    "    # Fetch the URL data using requests.get(url),\n",
    "    # store it in a variable, request_result.\n",
    "    request_result=requests.get(url)\n",
    "    \n",
    "    # Creating soup from the fetched request\n",
    "    soup = bs4.BeautifulSoup(request_result.text,\n",
    "                            \"html.parser\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    all_result = soup.find_all('div', class_=\"BNeawe s3v9rd AP7Wnd\")\n",
    "    context = \".\".join([all_result[i].text for i in range(10)])\n",
    "    \n",
    "    return context\n",
    "\n",
    "# suummarize the google search result\n",
    "def summary_context(tokens,context):\n",
    "    context = f'The definition of {tokens} is that:'+context\n",
    "\n",
    "    summary_text = summarizer(context, max_length=300, min_length=10, do_sample=False)[0]['summary_text']\n",
    "    return summary_text\n",
    "\n",
    "# make sure whether the token is related to the crypto\n",
    "def final_sure(tokens,summary_text):\n",
    "\n",
    "    related_list = ['bitcoin','btc','ethereum','crypto','cryptocurrency','blockchain','digitalcoin','digital currency','digital asset','digital']\n",
    "    def_corpus = f\"The definication of {tokens} is related to: \"+\" or \".join(related_list)\n",
    "    sentences = [def_corpus, summary_text]\n",
    "\n",
    "    #Encode all sentences\n",
    "    embeddings = model_sentence_sim.encode(sentences)\n",
    "\n",
    "    #Compute cosine similarity between all pairs\n",
    "    cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "    sim_score = float(cos_sim[0][1])\n",
    "\n",
    "    sim_thred = 0.5\n",
    "    if sim_score>sim_thred:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# select tokens that only are related to the crypto \n",
    "def token_is_crypto(tag_text):\n",
    "    crypto_tokens = []\n",
    "    for i in range(0, len(tag_text)):\n",
    "        tokens = tag_text[i]\n",
    "        \n",
    "        context = google_search(tokens)\n",
    "        \n",
    "        summary_text = summary_context(tokens,context)\n",
    "\n",
    "        if final_sure(tokens,summary_text):\n",
    "            crypto_tokens.append(tokens)\n",
    "    return crypto_tokens\n",
    "    \n",
    "# for i in range(len(title)):\n",
    "\n",
    "def labeled_title(each_title,each__time):\n",
    "\n",
    "    t = datetime.datetime.strptime(each_time,\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # make sentence\n",
    "    sentence = Sentence(each_title)\n",
    "\n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    all_tag = np.array([i.tag for i in sentence.get_spans('pos')])\n",
    "    all_text = np.array([i.text for i in sentence.get_spans('pos')])\n",
    "        \n",
    "    tag_text = []\n",
    "    import_lb = ['NNP','NNPS',\"NN\"]\n",
    "\n",
    "    for _tag,_text in zip(all_tag,all_text):\n",
    "        if _tag in import_lb:\n",
    "            tag_text.append(_text)\n",
    "            \n",
    "    crypto_tokens = token_is_crypto(tag_text)\n",
    "    \n",
    "    print(colored(\"Time:\",'yellow'), t)\n",
    "    print(colored(\"News title: \",'yellow'),each_title)\n",
    "    print(\"Keywords: \",tag_text)\n",
    "    print(colored(\"Crypto Label: \",'yellow'),colored(crypto_tokens,'green'))\n",
    "    print(\"--------------------------------------------------\")\n",
    "    # clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 300, but you input_length is only 187. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    }
   ],
   "source": [
    "crypto_tokens = token_is_crypto(tag_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data = pd.read_csv('sample_cryoto_news.csv',index_col=0)\n",
    "\n",
    "title = ori_data['title']\n",
    "create_time = ori_data['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 300, but you input_length is only 47. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 300, but you input_length is only 64. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 300, but you input_length is only 101. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 300, but you input_length is only 41. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 300, but you input_length is only 49. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTime:\u001b[0m 2021-12-04 12:30:05\n",
      "\u001b[33mNews title: \u001b[0m Bitcoin, Ethereum face largest correction since 19 May; is it time to buy the dip\n",
      "Keywords:  ['Bitcoin', 'Ethereum', 'face', 'correction', 'May', 'time', 'dip']\n",
      "\u001b[33mCrypto Label: \u001b[0m \u001b[32m['Bitcoin', 'Ethereum', 'face', 'correction', 'May', 'time', 'dip']\u001b[0m\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 4,\n",
    "i = 5\n",
    "each_title = ori_data['title'][i]\n",
    "each_time = ori_data['created_at'][i]\n",
    "labeled_title(each_title,each_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"Ethereum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = google_search(tokens)      \n",
    "summary_text = summary_context(tokens,context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ethereum is a blockchain platform with its own cryptocurrency, Ether (ETH) or Ethereum, and its own programming language, called Solidity.'"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context\n",
    "summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_list = ['bitcoin','btc','ethereum','crypto','cryptocurrency','blockchain','digitalcoin','digital currency','digital asset']\n",
    "def_corpus = f\"The definication is that: \"+\" or \".join(related_list)\n",
    "sentences = [def_corpus, summary_text]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model_sentence_sim.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "sim_score = float(cos_sim[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5591626167297363"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_score"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "528a031830962f4f937ffd0c092b889845b0f19378c058dbb596312c8f6eb8bb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
