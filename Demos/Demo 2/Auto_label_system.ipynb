{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================flair models=================================\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/pos-english\")\n",
    "\n",
    "# ==========================transformers models=============================\n",
    "from transformers import pipeline,AutoTokenizer, AutoModelForSeq2SeqLM,AutoModelForQuestionAnswering\n",
    "\n",
    "# Summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
    "\n",
    "# ======================sentence_transformers models=========================\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model_sentence_sim = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import bs4\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the keywords in google\n",
    "def google_search(tokens):\n",
    "    \n",
    "    text= f\"What is {tokens}?\"\n",
    "    url = 'https://google.com/search?q=' + text\n",
    "    \n",
    "    # Fetch the URL data using requests.get(url),\n",
    "    # store it in a variable, request_result.\n",
    "    request_result=requests.get(url)\n",
    "    \n",
    "    # Creating soup from the fetched request\n",
    "    soup = bs4.BeautifulSoup(request_result.text,\n",
    "                            \"html.parser\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    all_result = soup.find_all('div', class_=\"BNeawe s3v9rd AP7Wnd\")\n",
    "    context = \".\".join([all_result[i].text for i in range(2)])\n",
    "    # print(tokens,context)\n",
    "    return context\n",
    "\n",
    "# suummarize the google search result\n",
    "def summary_context(tokens,context):\n",
    "    context = f'The definition of {tokens} is that:'+context\n",
    "\n",
    "    summary_text = summarizer(context, max_length=300, min_length=10, do_sample=False)[0]['summary_text']\n",
    "    return summary_text\n",
    "\n",
    "# make sure whether the token is related to the crypto\n",
    "def final_sure(tokens,summary_text):\n",
    "\n",
    "    related_list = ['bitcoin','btc','ethereum','crypto','cryptocurrency','blockchain','digitalcoin','digital currency','digital asset','vitual currency']\n",
    "    def_corpus = f\"The definication is that: \"+\" or \".join(related_list)\n",
    "    sentences = [def_corpus, summary_text]\n",
    "\n",
    "    #Encode all sentences\n",
    "    embeddings = model_sentence_sim.encode(sentences)\n",
    "\n",
    "    #Compute cosine similarity between all pairs\n",
    "    cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "    sim_score = float(cos_sim[0][1])\n",
    "    \n",
    "    # ========== comment for this line==============\n",
    "    # print(colored(\"-similarity score-\",'yellow'),colored(tokens,'green'),sim_score)\n",
    "    \n",
    "    sim_thred = 0.4\n",
    "    if sim_score>sim_thred:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "  \n",
    "# select tokens that only are related to the crypto \n",
    "def token_is_crypto(tag_text):\n",
    "    crypto_tokens = []\n",
    "    for i in range(0, len(tag_text)):\n",
    "        tokens = tag_text[i]\n",
    "        \n",
    "        context = google_search(tokens)\n",
    "        \n",
    "        summary_text = summary_context(tokens,context)\n",
    "        \n",
    "        # ========== comment for this line==============\n",
    "        # print(colored('summarized text','yellow'),colored(tokens,'green'),summary_text)\n",
    "\n",
    "        if final_sure(tokens,summary_text):\n",
    "            crypto_tokens.append(tokens)\n",
    "    return crypto_tokens\n",
    "    \n",
    "\n",
    "def labeled_title(each_title,each__time):\n",
    "\n",
    "    t = datetime.datetime.strptime(each_time,\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # make sentence\n",
    "    sentence = Sentence(each_title)\n",
    "\n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    all_tag = np.array([i.tag for i in sentence.get_spans('pos')])\n",
    "    all_text = np.array([i.text for i in sentence.get_spans('pos')])\n",
    "        \n",
    "    tag_text = []\n",
    "    import_lb = ['NNP','NNPS',\"NN\"]\n",
    "\n",
    "    for _tag,_text in zip(all_tag,all_text):\n",
    "        if _tag in import_lb:\n",
    "            tag_text.append(_text)\n",
    "            \n",
    "    crypto_tokens = token_is_crypto(tag_text)\n",
    "    \n",
    "    print(colored(\"Time:\",'yellow'), t)\n",
    "    print(colored(\"News title: \",'yellow'),each_title)\n",
    "    print(\"Keywords: \",tag_text)\n",
    "    print(colored(\"Crypto Label: \",'yellow'),colored(crypto_tokens,'green'))\n",
    "    print(\"--------------------------------------------------\")\n",
    "    # clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_pre(lengthofnews):\n",
    "    for i in range(lengthofnews):\n",
    "        each_title = ori_data['title'][i]\n",
    "        each_time = ori_data['created_at'][i]\n",
    "        \n",
    "        labeled_title(each_title,each_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fullowing functions are for degging\n",
    "def sim_score(tokens,summary_text):\n",
    "    \n",
    "\n",
    "    related_list = ['bitcoin','btc','ethereum','crypto','cryptocurrency','blockchain','digitalcoin','digital currency','digital asset','vitual currency']\n",
    "    def_corpus = f\"The definication is that: \"+\" or \".join(related_list)\n",
    "    sentences = [def_corpus, summary_text]\n",
    "\n",
    "    #Encode all sentences\n",
    "    embeddings = model_sentence_sim.encode(sentences)\n",
    "\n",
    "    #Compute cosine similarity between all pairs\n",
    "    cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "    sim_score = float(cos_sim[0][1])\n",
    "    print(tokens,sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data = pd.read_csv('sample_cryoto_news.csv',index_col=0)\n",
    "\n",
    "title = ori_data['title']\n",
    "create_time = ori_data['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: noun\n",
      "a building or room containing collections of books, periodicals, and sometimes films and recorded music for people to read, borrow, or refer to.\"a school library\".noun\n",
      "\n",
      "summary_text: A glossary of terms from the BBC News website.\n",
      "sim_score: 0.3331325650215149\n"
     ]
    }
   ],
   "source": [
    "# =============================single tokens: debugging===============================\n",
    "\n",
    "tokens = \"Library\"\n",
    "context = google_search(tokens)      \n",
    "summary_text = summary_context(tokens,context)\n",
    "\n",
    "print(\"context:\",context)\n",
    "print(\"summary_text:\",summary_text)\n",
    "\n",
    "related_list = ['bitcoin','btc','ethereum','crypto','cryptocurrency','blockchain','digitalcoin','digital currency','digital asset','vitual currency']\n",
    "def_corpus = f\"The definication is that: \"+\" or \".join(related_list)\n",
    "sentences = [def_corpus, summary_text]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model_sentence_sim.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "sim_score = float(cos_sim[0][1])\n",
    "\n",
    "print(\"sim_score:\",sim_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for single:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTime:\u001b[0m 2021-12-04 12:30:05\n",
      "\u001b[33mNews title: \u001b[0m Bitcoin, Ethereum face largest correction since 19 May; is it time to buy the dip\n",
      "Keywords:  ['Bitcoin', 'Ethereum', 'face', 'correction', 'May', 'time', 'dip']\n",
      "\u001b[33mCrypto Label: \u001b[0m \u001b[32m['Bitcoin', 'Ethereum']\u001b[0m\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "each_title = ori_data['title'][i]\n",
    "each_time = ori_data['created_at'][i]\n",
    "labeled_title(each_title,each_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTime:\u001b[0m 2021-12-04 12:30:05\n",
      "\u001b[33mNews title: \u001b[0m $2.6 Billion Bug in Solana Program Library Disclosed: Details\n",
      "Keywords:  ['Bug', 'Solana', 'Program', 'Library']\n",
      "\u001b[33mCrypto Label: \u001b[0m \u001b[32m[]\u001b[0m\n",
      "--------------------------------------------------\n",
      "\u001b[33mTime:\u001b[0m 2021-12-04 12:30:05\n",
      "\u001b[33mNews title: \u001b[0m El Salvador Increases Bitcoin Holding To Over $60 Million, Tronâ€™s Justin Sun Scoops More BTC In Solidarity\n",
      "Keywords:  ['El', 'Salvador', 'Bitcoin', 'Holding', 'Tron', 'Justin', 'Sun', 'BTC', 'Solidarity']\n",
      "\u001b[33mCrypto Label: \u001b[0m \u001b[32m['Bitcoin', 'BTC']\u001b[0m\n",
      "--------------------------------------------------\n",
      "\u001b[33mTime:\u001b[0m 2021-12-04 12:30:05\n",
      "\u001b[33mNews title: \u001b[0m Shiba Inu Remains Top Holding in Whale Wallets After Market Drops by 25%\n",
      "Keywords:  ['Shiba', 'Inu', 'Holding', 'Whale', 'Market', '%']\n",
      "\u001b[33mCrypto Label: \u001b[0m \u001b[32m[]\u001b[0m\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# before run this function, please comment the related line code.\n",
    "demo_pre(3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "528a031830962f4f937ffd0c092b889845b0f19378c058dbb596312c8f6eb8bb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
